{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "1. What does R-squared represent in a regression model?\n",
        "\n",
        "ans- R-squared (R²) represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It indicates how well the model fits the data, ranging from 0 to 1.\n",
        "\n",
        "2. What are the assumptions of linear regression?\n",
        "\n",
        "ans- Linearity: The relationship between the independent and dependent variables is linear.\n",
        "Independence: The errors (residuals) are independent of each other.\n",
        "Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
        "Normality: The errors are normally distributed.\n",
        "No Multicollinearity: Independent variables are not highly correlated with each other.\n",
        "\n",
        "3. What is the difference between R-squared and Adjusted R-squared?\n",
        "\n",
        "ans- R-squared increases with the addition of more independent variables, even if they don't improve the model.\n",
        "Adjusted R-squared penalizes the inclusion of unnecessary variables, providing a more reliable measure of model fit, especially when comparing models with different numbers of predictors.\n",
        "\n",
        "4. Why do we use Mean Squared Error (MSE)?\n",
        "\n",
        "ans- MSE measures the average squared difference between the predicted and actual values. It's used to evaluate the model's prediction accuracy and to optimize model parameters by minimizing the error.\n",
        "\n",
        "5. What does an Adjusted R-squared value of 0.85 indicate?\n",
        "\n",
        "ans- An Adjusted R-squared of 0.85 indicates that 85% of the variance in the dependent variable is explained by the independent variables, adjusted for the number of predictors in the model. It suggests a strong fit.\n",
        "\n",
        "6. How do we check for normality of residuals in linear regression?\n",
        "\n",
        "ans- We can check for normality of residuals using:\n",
        "Histograms: To visualize the distribution of residuals.\n",
        "Q-Q plots: To compare the distribution of residuals to a normal distribution.\n",
        "Statistical tests: Like the Shapiro-Wilk test.\n",
        "\n",
        "7. What is multicollinearity, and how does it impact regression?\n",
        "\n",
        "ans- Multicollinearity is a high correlation between independent variables. It impacts regression by:\n",
        "Making coefficient estimates unstable.\n",
        "Reducing the statistical significance of predictors.\n",
        "Making it difficult to interpret the individual effects of predictors.\n",
        "\n",
        "8. What is Mean Absolute Error (MAE)?\n",
        "\n",
        "ans- MAE measures the average absolute difference between the predicted and actual values. It's less sensitive to outliers than MSE.\n",
        "\n",
        "9. What are the benefits of using an ML pipeline?\n",
        "\n",
        "ans- Benefits include:\n",
        "Automation: Streamlines the process from data preprocessing to model evaluation.\n",
        "Reproducibility: Ensures consistent results.\n",
        "Efficiency: Simplifies complex workflows.\n",
        "Scalability: Makes it easier to manage large datasets and complex models.\n",
        "\n",
        "10. Why is RMSE considered more interpretable than MSE?\n",
        "\n",
        "ans- RMSE (Root Mean Squared Error) is the square root of MSE, so it's in the same units as the dependent variable. This makes it easier to interpret the magnitude of the errors.\n",
        "\n",
        "11. What is pickling in Python, and how is it useful in ML?\n",
        "\n",
        "ans- Pickling is the process of serializing Python objects into a byte stream. It's useful in ML for:\n",
        "Saving trained models.\n",
        "Storing intermediate results.\n",
        "Transferring models between systems.\n",
        "\n",
        "12. What does a high R-squared value mean?\n",
        "\n",
        "ans- A high R-squared value (close to 1) means that the model explains a large proportion of the variance in the dependent variable, indicating a good fit.\n",
        "\n",
        "13. What happens if linear regression assumptions are violated?\n",
        "\n",
        "ans- Violating assumptions can lead to:\n",
        "Biased or inefficient coefficient estimates.\n",
        "Incorrect statistical inferences.\n",
        "Reduced model reliability.\n",
        "\n",
        "14. How can we address multicollinearity in regression?\n",
        "\n",
        "ans- We can address multicollinearity by:\n",
        "Removing one of the correlated variables.\n",
        "Using dimensionality reduction techniques (e.g., PCA).\n",
        "Using regularization (e.g., Ridge or Lasso regression).\n",
        "\n",
        "15. How can feature selection improve model performance in regression analysis?\n",
        "\n",
        "ans- Feature selection can:\n",
        "Reduce overfitting.\n",
        "Improve model interpretability.\n",
        "Reduce computational cost.\n",
        "Enhance generalization performance.\n",
        "\n",
        "16. How is Adjusted R-squared calculated?\n",
        "\n",
        "ans- Adjusted R-squared is calculated as: 1 - [(1 - R²) * (n - 1) / (n - k - 1)], where n is the number of observations and k is the number of predictors.\n",
        "\n",
        "17. Why is MSE sensitive to outliers?\n",
        "\n",
        "ans- MSE squares the errors, so large errors (caused by outliers) have a disproportionately large impact on the overall error.\n",
        "\n",
        "18. What is the role of homoscedasticity in linear regression?\n",
        "\n",
        "ans- Homoscedasticity ensures that the variance of the errors is constant across all levels of the independent variables, which is necessary for reliable statistical inference.\n",
        "\n",
        "19. What is Root Mean Squared Error (RMSE)?\n",
        "\n",
        "ans- RMSE is the square root of MSE, providing a measure of the average magnitude of the errors in the same units as the dependent variable.  \n",
        "\n",
        "20. Why is pickling considered risky?\n",
        "\n",
        "ans- Pickling can be risky because:\n",
        "It can execute arbitrary code during unpickling, posing a security risk.\n",
        "It's not guaranteed to be compatible across different Python versions.\n",
        "\n",
        "21. What alternatives exist to pickling for saving ML models?\n",
        "\n",
        "ans- Alternatives include:\n",
        "Joblib: For efficient serialization of NumPy arrays and large objects.\n",
        "ONNX (Open Neural Network Exchange): For interoperability between different ML frameworks.\n",
        "Cloud-based model storage: Using services like AWS S3 or Google Cloud Storage.\n",
        "\n",
        "22. What is heteroscedasticity, and why is it a problem?\n",
        "\n",
        "ans- Heteroscedasticity is the condition where the variance of the errors is not constant. It's a problem because it can lead to unreliable coefficient estimates and incorrect statistical inferences.\n",
        "\n",
        "23. How can interaction terms enhance a regression model's predictive power?\n",
        "\n",
        "ans- Interaction terms can capture how the effect of one independent variable on the dependent variable changes depending on the value of another independent variable, improving the model's ability to capture complex relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "3pU0gUgoi8qV"
      }
    }
  ]
}